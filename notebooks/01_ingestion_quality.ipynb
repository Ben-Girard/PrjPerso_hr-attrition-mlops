{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a5a425",
   "metadata": {},
   "source": [
    "# Analyse des causes d’attrition chez TechNova Partners  \n",
    "## 1. Ingestion et contrôle qualité des données\n",
    "\n",
    "**Projet :** HR Analytics – Identification des causes de démission  \n",
    "**Rôle :** Consultant Data Scientist  \n",
    "**Objectif du notebook :**  \n",
    "Construire un DataFrame central propre et cohérent à partir de trois sources RH hétérogènes, en réalisant les premiers contrôles qualité indispensables avant toute analyse exploratoire ou modélisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac03cb75",
   "metadata": {},
   "source": [
    "## Contexte et problématique\n",
    "\n",
    "TechNova Partners fait face à un taux de turnover plus élevé qu’habituellement.  \n",
    "Le département RH souhaite identifier les causes potentielles de ces départs afin de mettre en place des leviers d’action ciblés.\n",
    "\n",
    "Trois sources de données internes sont mises à disposition :\n",
    "\n",
    "1. **SIRH** : informations contractuelles, sociodémographiques et professionnelles\n",
    "2. **Évaluations annuelles** : performance et satisfaction\n",
    "3. **Sondage interne** : bien-être, perception, et variable cible de démission\n",
    "\n",
    "Avant toute analyse ou modélisation, il est indispensable de :\n",
    "- comprendre la structure de chaque source,\n",
    "- vérifier la qualité des données,\n",
    "- identifier les clés de jointure,\n",
    "- construire un jeu de données central cohérent et anonymisé.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa82bddb",
   "metadata": {},
   "source": [
    "## Démarche méthodologique\n",
    "\n",
    "La démarche suivie dans ce notebook est la suivante :\n",
    "\n",
    "1. Chargement des trois fichiers CSV bruts\n",
    "2. Contrôle qualité initial (dimensions, types, valeurs manquantes, doublons)\n",
    "3. Harmonisation des clés de jointure\n",
    "4. Jointure des trois sources\n",
    "5. Contrôles post-jointure\n",
    "6. Anonymisation des identifiants employés\n",
    "7. Sauvegarde d’un dataset central prêt pour l’analyse exploratoire\n",
    "\n",
    "Les transformations récurrentes sont implémentées dans des fonctions Python réutilisables (dossier `src/`) afin de garantir :\n",
    "- la reproductibilité,\n",
    "- la lisibilité,\n",
    "- et la maintenabilité du projet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d647d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from technova_attrition.config import PATHS\n",
    "from technova_attrition.data_io import (\n",
    "    anonymize_employee_id,\n",
    "    check_duplicates,\n",
    "    join_sources,\n",
    "    load_eval,\n",
    "    load_sirh,\n",
    "    load_sondage,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df3e82",
   "metadata": {},
   "source": [
    "## Chargement des données brutes\n",
    "\n",
    "Les fichiers CSV sont stockés dans le dossier `data/raw`.  \n",
    "Ils correspondent aux extractions brutes fournies par le responsable SIRH.\n",
    "\n",
    "À ce stade, aucune transformation métier n’est appliquée.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc90f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "CSV not found: G:\\Mon Drive\\OC\\Projet_4\\technova_attrition\\data\\raw\\Extrait SIRH.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m eval_path = PATHS.data_raw / \u001b[33m\"\u001b[39m\u001b[33mExtrait évaluations de performance.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m sondage_path = PATHS.data_raw / \u001b[33m\"\u001b[39m\u001b[33mExtrait sondage.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m sirh_df = \u001b[43mload_sirh\u001b[49m\u001b[43m(\u001b[49m\u001b[43msirh_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m eval_df = load_eval(eval_path)\n\u001b[32m      7\u001b[39m sondage_df = load_sondage(sondage_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\Mon Drive\\OC\\Projet_4\\technova_attrition\\src\\technova_attrition\\data_io.py:48\u001b[39m, in \u001b[36mload_sirh\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_sirh\u001b[39m(path: Path) -> pd.DataFrame:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     df = \u001b[43m_read_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# Expected join key already numeric: id_employee\u001b[39;00m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mid_employee\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df.columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\Mon Drive\\OC\\Projet_4\\technova_attrition\\src\\technova_attrition\\data_io.py:15\u001b[39m, in \u001b[36m_read_csv\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_csv\u001b[39m(path: Path) -> pd.DataFrame:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.exists():\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCSV not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.read_csv(path, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m, sep=\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m, low_memory=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: CSV not found: G:\\Mon Drive\\OC\\Projet_4\\technova_attrition\\data\\raw\\Extrait SIRH.csv"
     ]
    }
   ],
   "source": [
    "sirh_path = PATHS.data_raw / \"extrait_sirh.csv\"\n",
    "eval_path = PATHS.data_raw / \"extrait_eval.csv\"\n",
    "sondage_path = PATHS.data_raw / \"extrait_sondage.csv\"\n",
    "\n",
    "sirh_df = load_sirh(sirh_path)\n",
    "eval_df = load_eval(eval_path)\n",
    "sondage_df = load_sondage(sondage_path)\n",
    "\n",
    "sirh_df.shape, eval_df.shape, sondage_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96819512",
   "metadata": {},
   "source": [
    "## Inspection initiale des jeux de données\n",
    "\n",
    "Nous examinons pour chaque source :\n",
    "- les dimensions (nombre de lignes et colonnes),\n",
    "- les types de variables,\n",
    "- un aperçu des premières lignes.\n",
    "\n",
    "Cette étape permet de détecter rapidement :\n",
    "- des incohérences de typage,\n",
    "- des colonnes inutiles,\n",
    "- des valeurs manifestement aberrantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028104a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sirh_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb228e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sirh_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89270302",
   "metadata": {},
   "source": [
    "## Recherche de doublons\n",
    "\n",
    "Avant toute jointure, il est essentiel de vérifier l’unicité des identifiants métiers.\n",
    "\n",
    "Nous recherchons ici :\n",
    "- des doublons stricts (lignes identiques),\n",
    "- des doublons sur les clés de jointure potentielles.\n",
    "\n",
    "Ces vérifications permettent d’éviter des duplications artificielles lors des jointures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8070bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(sirh_df, subset=[\"id_employee\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba4346",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(eval_df, subset=[\"eval_number\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22566e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(sondage_df, subset=[\"code_sondage\"]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c48930",
   "metadata": {},
   "source": [
    "## Harmonisation des clés de jointure\n",
    "\n",
    "Les trois sources ne partagent pas une clé strictement identique :\n",
    "\n",
    "- **SIRH** : `id_employee` (entier)\n",
    "- **Évaluations** : `eval_number` (ex: \"E_1\", \"E_2\", ...)\n",
    "- **Sondage** : `code_sondage` (entier)\n",
    "\n",
    "Une transformation est nécessaire pour rendre ces clés compatibles :\n",
    "- extraction de la partie numérique de `eval_number`\n",
    "- création d’une clé numérique `eval_number_int`\n",
    "\n",
    "Cette étape est indispensable pour garantir des jointures fiables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7393aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[[\"eval_number\", \"eval_number_int\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80346a2e",
   "metadata": {},
   "source": [
    "## Jointure des trois sources\n",
    "\n",
    "Une jointure **inner** est utilisée afin de :\n",
    "- conserver uniquement les employés présents dans les trois systèmes,\n",
    "- garantir la cohérence du dataset final.\n",
    "\n",
    "Cette décision est justifiée dans un contexte d’analyse explicative, où la qualité prime sur l’exhaustivité.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef940873",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = join_sources(sirh_df, eval_df, sondage_df, how=\"inner\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1138ceff",
   "metadata": {},
   "source": [
    "## Contrôles qualité post-jointure\n",
    "\n",
    "Après la jointure, nous vérifions :\n",
    "- l’absence de duplication inattendue,\n",
    "- la cohérence des dimensions,\n",
    "- la présence effective de la variable cible.\n",
    "\n",
    "Ces contrôles permettent de s’assurer que la jointure n’a pas introduit d’erreurs structurelles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bfb0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610168e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e0991e",
   "metadata": {},
   "source": [
    "## Anonymisation des identifiants employés\n",
    "\n",
    "Les données RH sont **hautement sensibles**.  \n",
    "Avant toute analyse ou modélisation, les identifiants employés doivent être anonymisés.\n",
    "\n",
    "Méthode utilisée :\n",
    "- **HMAC-SHA256** avec clé secrète\n",
    "- anonymisation stable (un même employé garde le même identifiant anonymisé)\n",
    "- non réversible sans la clé\n",
    "\n",
    "Cette approche est conforme aux bonnes pratiques RGPD en contexte analytique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef68c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"employee_id_anon\"] = anonymize_employee_id(df[\"id_employee\"])\n",
    "df = df.drop(columns=[\"id_employee\"])\n",
    "\n",
    "df[[\"employee_id_anon\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b669085a",
   "metadata": {},
   "source": [
    "## Sauvegarde du dataset central\n",
    "\n",
    "Le DataFrame final est sauvegardé dans le dossier `data/processed`.\n",
    "\n",
    "Ce fichier constitue :\n",
    "- l’entrée unique pour l’analyse exploratoire,\n",
    "- la base de travail pour le feature engineering,\n",
    "- la référence utilisée pour la partie SQL et la modélisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = PATHS.data_processed / \"employees_joined.parquet\"\n",
    "df.to_parquet(output_path, index=False)\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b4258",
   "metadata": {},
   "source": [
    "## Conclusion de l’étape d’ingestion\n",
    "\n",
    "À l’issue de ce notebook, nous disposons :\n",
    "- d’un dataset central issu de trois sources RH,\n",
    "- de données contrôlées et cohérentes,\n",
    "- d’identifiants employés anonymisés,\n",
    "- d’un fichier prêt pour l’analyse exploratoire et statistique.\n",
    "\n",
    "La prochaine étape consistera à :\n",
    "- analyser les distributions des variables,\n",
    "- comparer les employés ayant quitté l’entreprise à ceux encore présents,\n",
    "- identifier les premiers signaux explicatifs de l’attrition.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "technova_attrition (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
